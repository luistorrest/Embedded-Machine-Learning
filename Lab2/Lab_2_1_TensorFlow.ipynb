{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embedded ML - Lab 2.1: TensorFlow"
      ],
      "metadata": {
        "id": "7SFBFiQlYlva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you will learn the basics of one of the most developed and widely used ML libraries: TensorFlow. It implements many of the most important ML models and algorithms and has optimized back-ends for efficient execution on CPUs, GPUs, TPUs and other devices.\n",
        "\n",
        "In this lab you are given some helper functions but you are expected to write most of the code and be able to explain it at a high level of abstraction and also to modify any part of it. This lab is important because a significant part of the course will use TensorFlow."
      ],
      "metadata": {
        "id": "svldvvGfmN8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts associated with TensorFlow\n",
        "* Use the basic workflow of TensorFlow to build a simple ML model\n",
        "* Implement simple dense networks with TensorFlow and Keras\n",
        "* Use some of the input handling functions of TensorFlow\n",
        "* Implement a simple CNN with TensorFlow and Keras"
      ],
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow workflow\n",
        "As in general with ML, in TensorFlow you have to get or preprocess the model inputs, train the model, run inference and evaluate results.\n",
        "\n",
        "Here you should use TensorFlow to build a dense 4-layer network to classify items in the FASHION MNIST dataset. Explore a few different hidden-layer sizes and report the accuracy achieved.\n",
        "\n",
        "Finally, export the model to a file and write a separate code that is able to load that model and run inference again."
      ],
      "metadata": {
        "id": "l8wat6Kxul5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "qmtQIbXA-Nmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process input dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Create the model\n",
        "def create_model(\n",
        "    hidden_layer_one : int = 128,\n",
        "    hidden_layer_two : int = 128,\n",
        "    hidden_layer_three : int = 128,\n",
        "    learnig_rate : float = 0.001\n",
        "    ):\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    keras.layers.Dense(hidden_layer_one, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(hidden_layer_two, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(hidden_layer_three, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "  ])\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learnig_rate),\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Train the model\n",
        "model = create_model()\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Evaluate functional performance\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(\"test lost: \", test_loss, \"\\ntest acc: \", test_acc)\n"
      ],
      "metadata": {
        "id": "JRV3aXGLTvE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69d72e1-af96-48f7-96bc-edc1d9f24529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 2s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 3ms/step - loss: 1.2767 - accuracy: 0.7513\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5309 - accuracy: 0.8171\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4704 - accuracy: 0.8339\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4292 - accuracy: 0.8459\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4087 - accuracy: 0.8540\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4561 - accuracy: 0.8392\n",
            "test lost:  0.45611512660980225 \n",
            "test acc:  0.8392000198364258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to a file\n",
        "model.save_weights(\"mi_primera_red\")"
      ],
      "metadata": {
        "id": "P7O69HR4DGsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create models\n",
        "models = {}\n",
        "models_performance = {}\n",
        "arquitecture = [\n",
        "    (128, 256, 64, 0.001),\n",
        "    (64, 128, 256, 0.001),\n",
        "    (256, 64, 128, 0.001),\n",
        "    (128, 64, 32, 0.001),\n",
        "    (64, 64, 32, 0.001),\n",
        "    (32, 64, 32, 0.001),\n",
        "    (256,256,256, 0.001)\n",
        "    ]\n",
        "for i in range(1, len(arquitecture)+1):\n",
        "  print(f\"#----------------------modelo_{i}--------------------#\")\n",
        "  models[f\"modelo_{i}\"] = create_model(\n",
        "      arquitecture[i-1][0],\n",
        "      arquitecture[i-1][1],\n",
        "      arquitecture[i-1][2],\n",
        "      arquitecture[i-1][3]\n",
        "      )\n",
        "  models[f\"modelo_{i}\"].summary()\n",
        "  models[f\"modelo_{i}\"].fit(train_images, train_labels, epochs=5)\n",
        "  test_loss, test_acc = models[f\"modelo_{i}\"].evaluate(test_images, test_labels)\n",
        "  models_performance[f\"modelo_{i}\"] = (test_loss, test_acc)\n",
        "  print(\"test lost: \", test_loss, \"\\ntest acc: \", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzfwxwhyLGwI",
        "outputId": "14b41a4c-9810-4a57-8e33-d8c16ca84140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#----------------------modelo_1--------------------#\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150602 (588.29 KB)\n",
            "Trainable params: 150602 (588.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.0662 - accuracy: 0.7587\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4985 - accuracy: 0.8254\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4379 - accuracy: 0.8439\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4104 - accuracy: 0.8550\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3904 - accuracy: 0.8598\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4591 - accuracy: 0.8453\n",
            "test lost:  0.45906367897987366 \n",
            "test acc:  0.845300018787384\n",
            "#----------------------modelo_2--------------------#\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                50240     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 94154 (367.79 KB)\n",
            "Trainable params: 94154 (367.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.0688 - accuracy: 0.6928\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6264 - accuracy: 0.7638\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5508 - accuracy: 0.7915\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5171 - accuracy: 0.8025\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4958 - accuracy: 0.8082\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5697 - accuracy: 0.7761\n",
            "test lost:  0.569682776927948 \n",
            "test acc:  0.7760999798774719\n",
            "#----------------------modelo_3--------------------#\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 227018 (886.79 KB)\n",
            "Trainable params: 227018 (886.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.2622 - accuracy: 0.7530\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5268 - accuracy: 0.8160\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4791 - accuracy: 0.8313\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4329 - accuracy: 0.8458\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4124 - accuracy: 0.8529\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4235 - accuracy: 0.8556\n",
            "test lost:  0.4234577715396881 \n",
            "test acc:  0.8555999994277954\n",
            "#----------------------modelo_4--------------------#\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111146 (434.16 KB)\n",
            "Trainable params: 111146 (434.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.2959 - accuracy: 0.7596\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5047 - accuracy: 0.8234\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4514 - accuracy: 0.8414\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4268 - accuracy: 0.8477\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3998 - accuracy: 0.8580\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4467 - accuracy: 0.8458\n",
            "test lost:  0.44674092531204224 \n",
            "test acc:  0.84579998254776\n",
            "#----------------------modelo_5--------------------#\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 64)                50240     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56810 (221.91 KB)\n",
            "Trainable params: 56810 (221.91 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.5867 - accuracy: 0.7332\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5992 - accuracy: 0.7990\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5159 - accuracy: 0.8227\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4849 - accuracy: 0.8313\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4444 - accuracy: 0.8445\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4647 - accuracy: 0.8364\n",
            "test lost:  0.4647466838359833 \n",
            "test acc:  0.8363999724388123\n",
            "#----------------------modelo_6--------------------#\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 32)                25120     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29642 (115.79 KB)\n",
            "Trainable params: 29642 (115.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 3ms/step - loss: 0.9923 - accuracy: 0.7147\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.5693 - accuracy: 0.7993\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5148 - accuracy: 0.8162\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4749 - accuracy: 0.8291\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4550 - accuracy: 0.8364\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.8184\n",
            "test lost:  0.5000714063644409 \n",
            "test acc:  0.8184000253677368\n",
            "#----------------------modelo_7--------------------#\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_7 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 335114 (1.28 MB)\n",
            "Trainable params: 335114 (1.28 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.3761 - accuracy: 0.7734\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4870 - accuracy: 0.8288\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4347 - accuracy: 0.8446\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4098 - accuracy: 0.8539\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3804 - accuracy: 0.8638\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.4923 - accuracy: 0.8300\n",
            "test lost:  0.49231085181236267 \n",
            "test acc:  0.8299999833106995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in models_performance:\n",
        "  print(f\"{key} loss: {models_performance[key][0]} ; acc: {models_performance[key][1]}\")\n",
        "  models[key].save_weights(f\"{key}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Ohmun6Urru",
        "outputId": "c142110e-4105-42ec-f688-2f90fbfc174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modelo_1 loss: 0.45906367897987366 ; acc: 0.845300018787384\n",
            "modelo_2 loss: 0.569682776927948 ; acc: 0.7760999798774719\n",
            "modelo_3 loss: 0.4234577715396881 ; acc: 0.8555999994277954\n",
            "modelo_4 loss: 0.44674092531204224 ; acc: 0.84579998254776\n",
            "modelo_5 loss: 0.4647466838359833 ; acc: 0.8363999724388123\n",
            "modelo_6 loss: 0.5000714063644409 ; acc: 0.8184000253677368\n",
            "modelo_7 loss: 0.49231085181236267 ; acc: 0.8299999833106995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "new_model = create_model()\n",
        "# Pre-process test inputs\n",
        "new_model.load_weights(\"mi_primera_red\")\n",
        "# Verify functional performance\n",
        "test_loss, test_acc = new_model.evaluate(test_images, test_labels)\n",
        "print(\"test lost: \", test_loss, \"\\ntest acc: \", test_acc)"
      ],
      "metadata": {
        "id": "IMZicM-FUgBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad3385d-578b-4813-d06b-1ba41a46750b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4561 - accuracy: 0.8392\n",
            "test lost:  0.45611512660980225 \n",
            "test acc:  0.8392000198364258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNNs with TensorFlow\n",
        "Convolutional Neural Networks add another type of processing layers to extract image features that allow the model to indentify patterns for a much better accuracy results in computer vision applications.\n",
        "\n",
        "Implement a CNN model to classify the FASHION MNIST dataset and compare the accuracy results with the previous dense model. Also report a comparison of the model size measuring the saved model file size and through an analytical estimation."
      ],
      "metadata": {
        "id": "YpQD3Y7a_wFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process input dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images.reshape(60000,28,28,1)\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images.reshape(10000,28,28,1)\n",
        "test_images = test_images / 255.0\n",
        "# Create the CNN model\n",
        "\n",
        "model_conv = keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Compile the CNN model\n",
        "model_conv.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                loss = 'sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "model_conv.summary()\n",
        "# Train the model\n",
        "model_conv.fit(train_images, train_labels, epochs=10)\n",
        "# Evaluate functional performance\n",
        "test_loss, test_acc = model_conv.evaluate(test_images, test_labels)\n",
        "print(\"test lost: \", test_loss, \"\\ntest acc: \", test_acc)\n",
        "# Save the model to a file\n",
        "model_conv.save_weights(\"modelo_conv\")"
      ],
      "metadata": {
        "id": "yFRKgGFxVyyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c856c301-939c-4f9c-8167-99c6b5090551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 128)       1280      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        73792     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 281290 (1.07 MB)\n",
            "Trainable params: 281290 (1.07 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 4ms/step - loss: 0.6410 - accuracy: 0.7745\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4264 - accuracy: 0.8480\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3770 - accuracy: 0.8654\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3468 - accuracy: 0.8768\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3216 - accuracy: 0.8844\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3049 - accuracy: 0.8901\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2905 - accuracy: 0.8961\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2778 - accuracy: 0.9004\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2652 - accuracy: 0.9043\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2545 - accuracy: 0.9089\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2955 - accuracy: 0.8933\n",
            "test lost:  0.29549646377563477 \n",
            "test acc:  0.8932999968528748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_operations(layer):\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        num_filters = layer.filters\n",
        "        kernel_size = layer.kernel_size[0] * layer.kernel_size[1]\n",
        "        input_size = layer.input_shape[1] * layer.input_shape[2] * layer.input_shape[3]\n",
        "        output_size = layer.output_shape[1] * layer.output_shape[2] * layer.output_shape[3]\n",
        "        num_operations = input_size * output_size * kernel_size * num_filters * 2 + output_size * num_filters\n",
        "    elif isinstance(layer, tf.keras.layers.Dense):\n",
        "        input_size = layer.input_shape[1]\n",
        "        output_size = layer.output_shape[1]\n",
        "        num_operations = input_size * output_size * 2 + output_size\n",
        "    else:\n",
        "        num_operations = 0  # unsupported layer\n",
        "    return num_operations\n",
        "\n",
        "# Example usage:\n",
        "total_operations_CNN = sum(calculate_operations(layer) for layer in model_conv.layers)\n",
        "total_operations = sum(calculate_operations(layer) for layer in model.layers)\n",
        "print(\"Total operations for the model with out:\", total_operations)\n",
        "print(\"Total operations for the CNN model:\", total_operations_CNN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTTLfK6ubJyw",
        "outputId": "2c0cca90-b541-4227-c7cf-cf5d91164c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total operations for the model with out: 269194\n",
            "Total operations for the CNN model: 349291600522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model size\n",
        "def model_size(archivo1, archivo2):\n",
        "    tamano_archivo1 = os.path.getsize(archivo1)\n",
        "    print(f\"{archivo1} el tamaño es {tamano_archivo1}\")\n",
        "    tamano_archivo2 = os.path.getsize(archivo2)\n",
        "    print(f\"{archivo2} el tamaño es {tamano_archivo2}\")\n",
        "    # Comparamos los tamaños\n",
        "    if tamano_archivo1 > tamano_archivo2:\n",
        "        print(f\"{archivo1} es más grande que {archivo2}\")\n",
        "    elif tamano_archivo1 < tamano_archivo2:\n",
        "        print(f\"{archivo2} es más grande que {archivo1}\")\n",
        "    else:    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        print(f\"{archivo1} y {archivo2} tienen el mismo tamaño\")\n",
        "\n",
        "archivo1 = \"mi_primera_red.data-00000-of-00001\"\n",
        "archivo2 = \"modelo_conv.data-00000-of-00001\"\n",
        "\n",
        "model_size(archivo1, archivo2)"
      ],
      "metadata": {
        "id": "2KU54mIybUGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1aa5dd-bfa5-4fd0-f415-d86f4dcb516f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mi_primera_red.data-00000-of-00001 el tamaño es 1620777\n",
            "modelo_conv.data-00000-of-00001 el tamaño es 3378787\n",
            "modelo_conv.data-00000-of-00001 es más grande que mi_primera_red.data-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer learning and fine tuning\n",
        "When you want to build a model but do not have enough data or resources to train a network with the accuracy you need, it possible to use a model that has been pre-trained on a large dataset and fine tune it with the target (smaller)dataset to solve the target classification problem.\n",
        "\n",
        "Here you should use TensorFlow and Keras to download a pre-trained vision model from TensorFlow Hub (e.g. MobileNet V2), add a softmax classification layer and train it with a small subset of the Fashion MNIST dataset.\n",
        "\n",
        "Compare runtimes and Top-1 accuracy of the resulting model with the dense and convolutional models previously built."
      ],
      "metadata": {
        "id": "YSOVgWbvOOQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def format_image(image, label):\n",
        "    # Add an additional dimension for channels (grayscale images have 1 channel)\n",
        "    image = tf.expand_dims(image, axis=-1)\n",
        "    # Resize the image to (224, 224)\n",
        "    image = tf.image.resize(image, (224, 224)) / 255.0\n",
        "    image = tf.image.grayscale_to_rgb(\n",
        "    image, name=None\n",
        ")\n",
        "    return image, label\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images =train_images[1500:2100,:,:]\n",
        "train_labels =train_labels[1500:2100]\n",
        "test_images =test_images[:1000,:,:]\n",
        "test_labels =test_labels[:1000]\n",
        "\n",
        "print(f\"{train_images.shape} este el tamaño del la imagen\")\n",
        "\n",
        "# Since you are using fashion_mnist dataset, you can get the number of examples and classes directly\n",
        "num_examples = len(train_images)\n",
        "num_classes = len(np.unique(train_labels))\n",
        "print(num_examples)\n",
        "print(num_classes)\n",
        "\n",
        "# Convert your data into a tf.data.Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# split the data in training, validation, and test datasets\n",
        "BATCH_SIZE = 32\n",
        "train_batches = train_data.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = train_data.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = test_data.map(format_image).batch(1)\n",
        "\n",
        "# display the shape of our data\n",
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    pass\n",
        "print(f\"{image_batch.shape} imagen bath size\")\n",
        "\n",
        "# download the pre-trained model and create a Keras meta-layer\n",
        "\n",
        "module_selection = (\"mobilenet_v2\", 224, 1280)\n",
        "handle_base, pixels, FV_SIZE = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and output dimension {}\".format(MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))\n",
        "\n",
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,),\n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=False)\n",
        "\n",
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "\n",
        "# build a new model adding a softmax layer\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        tf.keras.layers.Dense(56, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile and train the new model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 5\n",
        "model.fit(train_batches,\n",
        "                 epochs=EPOCHS,\n",
        "                 validation_data=validation_batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sYkfBf0OSQj",
        "outputId": "18578ea8-23b0-4b8b-aa1e-1d9318bea017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n",
            "(600, 28, 28) este el tamaño del la imagen\n",
            "600\n",
            "10\n",
            "(32, 224, 224, 3) imagen bath size\n",
            "Using https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4 with input size (224, 224) and output dimension 1280\n",
            "Building model with https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 56)                71736     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                570       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2330290 (8.89 MB)\n",
            "Trainable params: 72306 (282.45 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "19/19 [==============================] - 10s 184ms/step - loss: 1.4487 - accuracy: 0.5267 - val_loss: 0.7416 - val_accuracy: 0.7833\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 1s 74ms/step - loss: 0.6210 - accuracy: 0.7883 - val_loss: 0.4425 - val_accuracy: 0.8433\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 1s 71ms/step - loss: 0.4078 - accuracy: 0.8533 - val_loss: 0.3162 - val_accuracy: 0.8883\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 1s 72ms/step - loss: 0.2965 - accuracy: 0.8950 - val_loss: 0.2236 - val_accuracy: 0.9317\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 2s 106ms/step - loss: 0.2306 - accuracy: 0.9267 - val_loss: 0.2106 - val_accuracy: 0.9300\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7837c5d920e0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "print(\"test lost: \", test_loss, \"\\ntest acc: \", test_acc)\n"
      ],
      "metadata": {
        "id": "vUoqfOcmR_3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bc9314-aa66-4d8d-f56c-5bd9573872de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/1000 [==============================] - 11s 10ms/step - loss: 0.5713 - accuracy: 0.7890\n",
            "test lost:  0.5712833404541016 \n",
            "test acc:  0.7889999747276306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"modelo_transfer_tunning\")"
      ],
      "metadata": {
        "id": "YAeh0RmvE918"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPc6Vo-pLVrg",
        "outputId": "9bacaf5f-09ca-4c16-b690-883508264ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 56)                71736     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                570       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2330290 (8.89 MB)\n",
            "Trainable params: 72306 (282.45 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The total number of operations for the dense model is 269,194.\n",
        "The total number of operations for the convolutional neural network (CNN) model is 349,291,600,522. This indicates significantly higher computational complexity compared to the dense model, which is expected due to the additional convolutional and pooling layers in the CNN architecture.\n",
        "Additionally, you've observed that the size of the saved weights for the CNN model is larger than that of the dense model. Specifically, the size of modelo_conv.data-00000-of-00001 is 3,378,787 bytes, while mi_primera_red.data-00000-of-00001 is 1,620,777 bytes.\n",
        "These observations highlight the trade-offs between model complexity, computational requirements, and storage space. While CNNs can achieve higher accuracy in certain tasks, they often require more computational resources and storage due to their deeper architectures and larger number of parameters.\n",
        "\n",
        "In this practice, we embarked on a journey through TensorFlow, a foundational tool in machine learning, to construct and train neural network models for image classification tasks using the Fashion MNIST dataset. Beginning with dense neural networks, we experimented with varying architectures and observed their impact on model accuracy. Transitioning to convolutional neural networks (CNNs), we witnessed their superior performance, attributed to their ability to capture spatial features in images. Through calculations of computational complexity and comparison of storage sizes, we grasped the trade-offs inherent in model design, balancing between accuracy and resource efficiency. This practice not only equipped us with practical skills in TensorFlow but also instilled a deeper understanding of fundamental concepts crucial for navigating the landscape of machine learning applications.\n",
        "\n",
        "Using pre-trained models is a very useful way to save time and computation, since not having to train the entire network allows testing different types of classifiers with more varieties in significantly less time.  However, the weight of the file “model_transfer_tunning.data-00000-of-00001” is 9,970,911 bytes, which is the heaviest so far, this is because the pre-trained network brings a lot of information that is not specific to this problem and has some more general convolutional layers, which are not all used. Pre-trained algorithms 'saves time but at the cost of more memory depending on the pre-trained network used.\n",
        "\n",
        "When deploying models in resource-constrained environments or considering storage limitations, it's essential to strike a balance between model complexity and performance. Techniques such as model compression, quantization, and architecture optimization can help mitigate these challenges while maintaining acceptable accuracy levels."
      ],
      "metadata": {
        "id": "aXMCb2YJijia"
      }
    }
  ]
}