{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SFBFiQlYlva"
      },
      "source": [
        "# Embedded ML - Lab 1.2: Model Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrantes\n",
        "Omar David Boneda\n",
        "\n",
        "Luis Fernando Torres"
      ],
      "metadata": {
        "id": "zUp6MA7mU31E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svldvvGfmN8q"
      },
      "source": [
        "In this lab you are asked to create a compressed verion of an ANNs model. You are not allowed to use ML libraries such as SciKit-learn, PyTorch or TensorFlow, but you are allowed to use standard libraries such as math, numpy and matplotlib if needed. You are given some code but you are expected to write some more and be able to explain and modify everything. This lab is essential for you to grasp the details of some of the most important techniques for compressing or making ML models more efficient: quantization and prunning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      },
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts of compression in ANNs\n",
        "* Apply range tuning and centering when doing quantization\n",
        "* Calculate and analyze the impact of quantization and prunning on memory and computing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wat6Kxul5R"
      },
      "source": [
        "### Naive quantization\n",
        "Quantization means reducing the precission of model parameters and mainly targets weights, since they represent the most volumne of memory and processing in ANNs.\n",
        "\n",
        "Take the code from the last part of Lab 1.1 (MNIST model) and add methods to export and import weights to and from a binary file for which you have to define a format. Then, create two additional inference methods: FP16 and INT8. The FP16 method should treat all computations in the network as 16-bit floating-point. The INT8 method should work with 8-bit integers. In both cases, use the native datatype conversion methods.\n",
        "\n",
        "Run the two quantized models and compare them with the baseline in terms of model size, accuracy and latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjFm3p9aMJGy",
        "outputId": "903aa523-edd0-47a7-8e97-258065755875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.2989\n",
            "Epoch 100: Loss = 0.0988\n",
            "Epoch 200: Loss = 0.0937\n",
            "Epoch 300: Loss = 0.0881\n",
            "Epoch 400: Loss = 0.0840\n",
            "Epoch 500: Loss = 0.0781\n",
            "Epoch 600: Loss = 0.0711\n",
            "Epoch 700: Loss = 0.0680\n",
            "Epoch 800: Loss = 0.0662\n",
            "Epoch 900: Loss = 0.0645\n",
            "Epoch 1000: Loss = 0.0571\n",
            "Epoch 1100: Loss = 0.0465\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.bias_input_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.bias_hidden_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def weights_flatten(self):\n",
        "        #Se le hace un flatten a los pesos\n",
        "        weights = np.concatenate((self.weights_input_hidden.flatten(), self.bias_input_hidden.flatten()))\n",
        "        weights = np.concatenate((weights,self.weights_hidden_output.flatten()))\n",
        "        weights = np.concatenate((weights,self.bias_hidden_output .flatten()))\n",
        "        return weights\n",
        "\n",
        "    def naive_quantized(self, x, cuantized):\n",
        "        if cuantized == 0:\n",
        "          #Cuantizado int 8 naive quantized\n",
        "          x_int8 = np.floor(((2**7)/np.max(x))* x).astype(np.int8)\n",
        "          weights_input_hidden_int8 = np.floor(((2**7)/np.max(self.weights_input_hidden))* self.weights_input_hidden).astype(np.int8)\n",
        "          bias_input_hidden_int8 = np.floor(((2**7)/np.max(self.bias_input_hidden))* self.bias_input_hidden).astype(np.int8)\n",
        "          weights_hidden_output_int8 = np.floor(((2**7)/np.max(self.weights_hidden_output))* self.weights_hidden_output).astype(np.int8)\n",
        "          bias_hidden_output_int8 = np.floor(((2**7)/np.max(self.bias_hidden_output))* self.bias_hidden_output).astype(np.int8)\n",
        "          name_int8 = 'model_weights_int8.npy'\n",
        "          self.save_weights_binary(name_int8,weights_input_hidden_int8,bias_input_hidden_int8,weights_hidden_output_int8,bias_hidden_output_int8)\n",
        "          return x_int8,weights_input_hidden_int8,bias_input_hidden_int8,weights_hidden_output_int8,bias_hidden_output_int8\n",
        "\n",
        "        elif cuantized == 1:\n",
        "          #Cuantizando int 8 tuning + centering\n",
        "          scale= (2**8)/(np.max(x)-np.min(x))\n",
        "          zeropoint=-np.floor(scale*np.min(x))-2**(8-1)\n",
        "          x_int8_c = np.floor((scale*x)+zeropoint).astype(np.int8)\n",
        "          weights_input_hidden_int8_c = np.floor((scale*weights_input_hidden)+zeropoint).astype(np.int8)\n",
        "          bias_input_hidden_int8_c = np.floor((scale*bias_input_hidden)+zeropoint).astype(np.int8)\n",
        "          weights_hidden_output_int8_c = np.floor((scale*weights_hidden_output)+zeropoint).astype(np.int8)\n",
        "          bias_hidden_output_int8_c = np.floor((scale*bias_hidden_output)+zeropoint).astype(np.int8)\n",
        "          name_int8_c = 'model_weights_int8_centering.npy'\n",
        "          self.save_weights_binary(name_int8_c,weights_input_hidden_int8_c,bias_input_hidden_int8_c,weights_hidden_output_int8_c,bias_hidden_output_int8_c)\n",
        "          return x_int8_c,weights_input_hidden_int8_c,bias_input_hidden_int8_c,weights_hidden_output_int8_c,bias_hidden_output_int8_c\n",
        "\n",
        "        elif cuantized == 2:\n",
        "          #Cuantizado fp16\n",
        "          self.save_weights_binary('model_weights_fp16.npy', self.weights_input_hidden.astype(np.float16), self.bias_input_hidden.astype(np.float16), self.weights_hidden_output.astype(np.float16), self.bias_hidden_output.astype(np.float16))\n",
        "          return  x.astype(np.float16),self.weights_input_hidden.astype(np.float16), self.bias_input_hidden.astype(np.float16), self.weights_hidden_output.astype(np.float16), self.bias_hidden_output.astype(np.float16)\n",
        "\n",
        "        else:\n",
        "          #sin cuantizar\n",
        "          self.save_weights_binary('model_weights.npy', self.weights_input_hidden, self.bias_input_hidden, self.weights_hidden_output, self.bias_hidden_output)\n",
        "          return  self.weights_input_hidden, self.bias_input_hidden, self.weights_hidden_output, self.bias_hidden_output\n",
        "\n",
        "    def save_weights_binary(self,name, weights_input_hidden,bias_input_hidden,weights_hidden_output,bias_hidden_output):\n",
        "      with open(name,'wb') as f:\n",
        "        np.save(f,weights_input_hidden)\n",
        "        np.save(f,bias_input_hidden)\n",
        "        np.save(f,weights_hidden_output)\n",
        "        np.save(f,bias_hidden_output)\n",
        "\n",
        "    def load_weights_binary(self, name):\n",
        "      with open(name, 'rb') as f:\n",
        "        self.weights_input_hidden = np.load(f)\n",
        "        self.bias_input_hidden = np.load(f)\n",
        "        self.weights_hidden_output = np.load(f)\n",
        "        self.bias_hidden_output = np.load(f)\n",
        "      return self.weights_input_hidden, self.bias_input_hidden, self.weights_hidden_output, self.bias_hidden_output\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def ReLU(self,x):\n",
        "        return np.abs(x * (x > 0))\n",
        "\n",
        "    def dReLU(self,x):\n",
        "        return np.abs(1. * (x > 0))\n",
        "\n",
        "    def active_neuron(self, y):\n",
        "        return np.argmax(y, axis=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward propagation through the network\n",
        "        self.hidden_output = self.sigmoid(np.dot(x, self.weights_input_hidden) + self.bias_input_hidden)\n",
        "        self.output = self.sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        # Backpropagation and weight updates\n",
        "        self.error = y - output\n",
        "        d_output = self.error * self.sigmoid_derivative(output)\n",
        "\n",
        "        self.hidden_error = d_output.dot(self.weights_hidden_output.T)\n",
        "        d_hidden = self.hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * learning_rate\n",
        "        self.bias_hidden_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += x.T.dot(d_hidden) * learning_rate\n",
        "        self.bias_input_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, x, y, epochs, learning_rate):\n",
        "        error = 0\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(x)\n",
        "            self.backward(x, y, output, learning_rate)\n",
        "            if epoch % 100 == 0:\n",
        "                error = np.mean(np.square(y - output))\n",
        "                print(f'Epoch {epoch}: Loss = {error:.4f}')\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten the training and test data\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "#y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "x_train = x_train[0:10000]\n",
        "y_train = y_train[0:10000]\n",
        "\n",
        "nn_mnist = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "nn_mnist.train(x_train, y_train, epochs=1200, learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pmqbqfkpMLuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd806e07-8150-45fb-e684-bbeed7c55038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision del modelo sin cuantificar 62.63\n"
          ]
        }
      ],
      "source": [
        "#test model sin cuantificar\n",
        "y_pred = nn_mnist.forward(x_test)\n",
        "y_pred = nn_mnist.active_neuron(y_pred)\n",
        "\n",
        "#Computation error\n",
        "error = y_test - y_pred\n",
        "print(\"Precision del modelo sin cuantificar\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2wDH_-cj945U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbedc324-e255-4f6f-b3d2-3317f51d5277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo sin cuantizar\n",
            "(784, 80) (1, 80) (80, 10) (1, 10)\n",
            "[-8.57963477e-02 -9.41172136e-01  7.35324649e-01  1.44057903e-01\n",
            "  1.44977344e-01 -2.82324914e-01 -1.71723027e+00  1.00988430e+00\n",
            " -2.98478555e-01 -1.16592557e+00  5.69461375e-01 -2.09088737e-01\n",
            " -1.16697645e-01 -2.37744796e-01  7.26117721e-01 -2.67035312e-02\n",
            " -4.32253406e-02 -2.20114891e+00  2.02569824e-02  1.02187981e+00\n",
            " -1.54003509e+00  2.18368243e-03  3.42998167e-01  1.43530360e-01\n",
            "  1.24316791e+00 -3.54641848e-01 -6.42608753e-01  8.56694167e-01\n",
            " -9.49031358e-01  1.69793687e+00]\n"
          ]
        }
      ],
      "source": [
        "#Cargar modelo con pesos normales\n",
        "weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output = nn_mnist.naive_quantized(x_test, 3)\n",
        "print(\"Modelo sin cuantizar\")\n",
        "print(weights_input_hidden.shape, bias_input_hidden.shape, weights_hidden_output.shape, bias_hidden_output.shape)\n",
        "print(weights_input_hidden[0,0:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XoIDFo-TCzm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f67ad46-d8c7-4082-c60f-9b86083b2aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cuantizado int8\n",
            "(784, 80) (1, 80) (80, 10) (1, 10)\n",
            "[ -3 -28  21   4   4  -9 -50  29  -9 -34  16  -7  -4  -7  20  -1  -2 -64\n",
            "   0  29 -45   0   9   4  35 -11 -19  24 -28  48]\n",
            "y_pred_int8:  int8\n",
            "y_pred_int8_active:  int8\n",
            "error int8:  int8\n",
            "Precision del modelo cuantificado int8 10.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-154c73705eae>:76: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ],
      "source": [
        "#Cargar modelo cuantizado int8\n",
        "x_test_int8, weights_input_hidden_int8, bias_input_hidden_int8, weights_hidden_output_int8, bias_hidden_output_int8 = nn_mnist.naive_quantized(x_test, 0)\n",
        "nn_mnist_int8 = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "weights_input_hidden_int8,bias_input_hidden_int8,weights_hidden_output_int8,bias_hidden_output_int8 = nn_mnist_int8.load_weights_binary('model_weights_int8.npy')\n",
        "print(\"Modelo cuantizado int8\")\n",
        "print(weights_input_hidden_int8.shape, bias_input_hidden_int8.shape, weights_hidden_output_int8.shape, bias_hidden_output_int8.shape)\n",
        "print(weights_input_hidden_int8[0,0:30])\n",
        "#Forward con int8\n",
        "y_pred_int8 = nn_mnist_int8.forward(x_test_int8).astype(np.int8)\n",
        "print(\"y_pred_int8: \",y_pred_int8.dtype)\n",
        "y_pred_int8 = nn_mnist_int8.active_neuron(y_pred_int8).astype(np.int8)\n",
        "print(\"y_pred_int8_active: \",y_pred_int8.dtype)\n",
        "#Computation error\n",
        "error = y_test.astype(np.int8) - y_pred_int8\n",
        "#error = y_test - y_pred_int8\n",
        "print(\"error int8: \",error.dtype)\n",
        "print(\"Precision del modelo cuantificado int8\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar modelo cuantizado float 16\n",
        "x_test_fp16, weights_input_hidden_fp16, bias_input_hidden_fp16, weights_hidden_output_fp16, bias_hidden_output_fp16 = nn_mnist.naive_quantized(x_test, 2)\n",
        "nn_mnist_fp16 = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "weights_input_hidden_fp16,bias_input_hidden_fp16,weights_hidden_output_fp16,bias_hidden_output_fp16 = nn_mnist_fp16.load_weights_binary('model_weights_fp16.npy')\n",
        "print(\"Modelo cuantizado fp16\")\n",
        "print(weights_input_hidden_fp16.shape, bias_input_hidden_fp16.shape, weights_hidden_output_fp16.shape, bias_hidden_output_fp16.shape)\n",
        "print(weights_input_hidden_fp16[0,0:30])\n",
        "#Forward con fp16\n",
        "y_pred_fp16 = nn_mnist_fp16.forward(x_test_fp16).astype(np.float16)\n",
        "print(\"y_pred_fp16: \",y_pred_fp16.dtype)\n",
        "y_pred_fp16 = nn_mnist_fp16.active_neuron(y_pred_fp16).astype(np.float16)\n",
        "print(\"y_pred_fp16_active: \",y_pred_fp16.dtype)\n",
        "#Computation error\n",
        "error = y_test.astype(np.float16) - y_pred_fp16\n",
        "#error = y_test - y_pred_fp16\n",
        "print(\"error fp16: \",error.dtype)\n",
        "print(\"Precision del modelo cuantificado float 16\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ],
      "metadata": {
        "id": "6pfcOETsWYvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d0a5bc-6ba0-417e-94e3-05b13c51bdc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cuantizado fp16\n",
            "(784, 80) (1, 80) (80, 10) (1, 10)\n",
            "[-8.582e-02 -9.414e-01  7.354e-01  1.440e-01  1.450e-01 -2.822e-01\n",
            " -1.717e+00  1.010e+00 -2.986e-01 -1.166e+00  5.693e-01 -2.091e-01\n",
            " -1.167e-01 -2.378e-01  7.261e-01 -2.670e-02 -4.321e-02 -2.201e+00\n",
            "  2.026e-02  1.021e+00 -1.540e+00  2.184e-03  3.430e-01  1.436e-01\n",
            "  1.243e+00 -3.547e-01 -6.426e-01  8.569e-01 -9.492e-01  1.698e+00]\n",
            "y_pred_fp16:  float16\n",
            "y_pred_fp16_active:  float16\n",
            "error fp16:  float16\n",
            "Precision del modelo cuantificado float 16 62.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-154c73705eae>:76: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpQD3Y7a_wFa"
      },
      "source": [
        "### Range tuning and centering\n",
        "For quantization to be effective, you should smartly choose the range of numbers you will code with the fewer bits available after quantization. To do so, you should evaluate the dynamic ranges of the variables to be quantized and map the values using that as the full range.\n",
        "\n",
        "Make a histogram plot of the model weights in order to verify their range. Then write a function to quantize the weights stored in the exported binary file to INT8 and store the resulting weights in another file. Finally, run again the INT8 quantized inference with the newly computed weights and compare with the previous versions using the same metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eDMywPk5qZiV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "6e8ebbfc-8e2a-458d-8e68-336b903af144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio -0.0031045671110856744\n",
            "Maximo: 4.438535142980115\n",
            "Mínimo: -4.692199240760835\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# histograma de pesos sin cuantificar\n",
        "def plot_histogram(weights):\n",
        "  print(\"Promedio\",np.mean(weights))\n",
        "  print(\"Maximo:\",np.max(weights))\n",
        "  print(\"Mínimo:\",np.min(weights))\n",
        "\n",
        "  # plot a histogram of all model weights\n",
        "  #plt.xticks([i for i in range(-5,5)])\n",
        "  plt.hist(weights,density=1,bins=100)\n",
        "  plt.grid()\n",
        "  plt.show\n",
        "\n",
        "#Histograma\n",
        "weights =nn_mnist.weights_flatten()\n",
        "plot_histogram(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cuantizar modelo int8 con centering\n",
        "x_test_int8_c, weights_input_hidden_int8_c, bias_input_hidden_int8_c, weights_hidden_output_int8_c, bias_hidden_output_int8_c = nn_mnist.naive_quantized(x_test, 1)\n",
        "\n",
        "#Cargar nuevo modelo con centering\n",
        "nn_mnist_int8_c = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "weights_input_hidden_int8_c,bias_input_hidden_int8_c,weights_hidden_output_int8_c,bias_hidden_output_int8_c = nn_mnist_int8_c.load_weights_binary('model_weights_int8_centering.npy')\n",
        "\n",
        "#Forward con int8 y cenetering\n",
        "y_pred_int8_c = nn_mnist_int8_c.forward(x_test_int8_c).astype(np.int8)\n",
        "print(\"y_pred_int8: \",y_pred_int8_c.dtype)\n",
        "y_pred_int8_c = nn_mnist_int8_c.active_neuron(y_pred_int8_c).astype(np.int8)\n",
        "print(\"y_pred_int8_active: \",y_pred_int8_c.dtype)\n",
        "\n",
        "#Computation error\n",
        "error = y_test.astype(np.int8) - y_pred_int8_c\n",
        "print(\"error int8 centering: \",error.dtype)\n",
        "print(\"Precision del modelo cuantificado int 8 con centering\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ],
      "metadata": {
        "id": "Y3n7cNRgWNyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180ac1b5-a106-4153-92db-3d62bb4cda6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred_int8:  int8\n",
            "y_pred_int8_active:  int8\n",
            "error int8 centering:  int8\n",
            "Precision del modelo cuantificado int 8 con centering 11.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-154c73705eae>:76: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#histograma de pesos cuantificados a int8 y centering\n",
        "weights =nn_mnist_int8_c.weights_flatten()\n",
        "plot_histogram(weights)"
      ],
      "metadata": {
        "id": "pPV0qZW1fqYX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "0d872b6d-ce52-462a-ccf5-cb6e3cc21f30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promedio -0.32936645181575225\n",
            "Maximo: 127\n",
            "Mínimo: -128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDh7MZVJEuhd"
      },
      "source": [
        "### Prunning\n",
        "Besides reducing precision for the network weights, we can also decide to eliminate network connections that do not contribute significantly to the model. This can be achieved by simply removing the connections whose weights are closest to zero.\n",
        "\n",
        "In this part of the lab you are asked to generate three prunned versions of the original model by setting to zero some of the weights:\n",
        "\n",
        "\n",
        "*   Set to zero the smallest 10% of weights\n",
        "*   Set to zero the smallest 30% of weights\n",
        "*   Set to zero the smallest 50% of weights\n",
        "\n",
        "Report the accuracy for each model against the estimated memory savings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_model(weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, percentage):\n",
        "\n",
        "    # Prune the input-to-hidden weights matrix\n",
        "    weights_input_hidden_prunned = prune_matrix(weights_input_hidden, percentage)\n",
        "    print(\"weights_input_hidden_prunned shape:\", weights_input_hidden_prunned.shape)\n",
        "\n",
        "    # Prune the input-to-hidden bias vector\n",
        "    bias_input_hidden_prunned = prune_vector(bias_input_hidden, percentage)\n",
        "    print(\"bias_input_hidden_prunned shape:\", bias_input_hidden_prunned.shape)\n",
        "\n",
        "    # Prune the hidden-to-output weights matrix\n",
        "    weights_hidden_output_prunned = prune_matrix(weights_hidden_output, percentage)\n",
        "    print(\"weights_hidden_output_prunned shape:\", weights_hidden_output_prunned.shape)\n",
        "\n",
        "    # Prune the hidden-to-output bias vector\n",
        "    bias_hidden_output_prunned = prune_vector(bias_hidden_output, percentage)\n",
        "    print(\"bias_hidden_output_prunned shape:\", bias_hidden_output_prunned.shape)\n",
        "\n",
        "    return weights_input_hidden_prunned, bias_input_hidden_prunned, weights_hidden_output_prunned, bias_hidden_output_prunned\n",
        "\n",
        "def prune_matrix(matrix, percentage):\n",
        "    # Calculate the number of elements to prune\n",
        "    num_elements_to_prune = int(matrix.size * percentage / 100)\n",
        "\n",
        "    # Find the indices of the smallest values\n",
        "    indices = np.argpartition(np.abs(matrix.flatten()), num_elements_to_prune)[:num_elements_to_prune]\n",
        "\n",
        "    # Convert indices to (x, y) coordinates\n",
        "    coordinates = [(index // matrix.shape[0], index % matrix.shape[1]) for index in indices]\n",
        "\n",
        "    # Set the smallest values to zero\n",
        "    for x, y in coordinates:\n",
        "        matrix[x, y] = 0\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def prune_vector(vector, percentage):\n",
        "    # Calculate the number of elements to prune\n",
        "    num_elements_to_prune = int(vector.size * percentage / 100)\n",
        "\n",
        "    # Find the indices of the smallest values\n",
        "    indices = np.argpartition(np.abs(vector), num_elements_to_prune)[:num_elements_to_prune]\n",
        "\n",
        "    # Set the smallest values to zero\n",
        "    for index in indices:\n",
        "        vector[index] = 0\n",
        "\n",
        "    return vector"
      ],
      "metadata": {
        "id": "mRCCzWGoQCa6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned model with 10%\n",
        "weights_input_hidden_p10, bias_input_hidden_p10, weights_hidden_output_p10, bias_hidden_output_p10 = prune_model(weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, 0.1)\n",
        "\n",
        "#Crear un nuevo objeto\n",
        "nn_mnist_p10 = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "nn_mnist_p10.weights_input_hidden = weights_input_hidden_p10\n",
        "nn_mnist_p10.bias_input_hidden_p10 = bias_input_hidden_p10\n",
        "nn_mnist_p10.weights_hidden_output_p10 = weights_hidden_output_p10\n",
        "nn_mnist_p10.bias_hidden_output_p10= bias_hidden_output_p10\n",
        "\n",
        "print(weights_input_hidden_p10.shape, bias_input_hidden_p10.shape, weights_hidden_output_p10.shape, bias_hidden_output_p10.shape)\n",
        "#test model\n",
        "y_pred_p10= nn_mnist_p10.forward(x_test)\n",
        "y_pred_p10 = nn_mnist_p10.active_neuron(y_pred_p10)\n",
        "\n",
        "#Computation error\n",
        "error = y_test - y_pred_p10\n",
        "print(\"Precision del modelo sin cuantificar y prunning 10%\",(np.count_nonzero(error==0)*100)/len(y_test))\n"
      ],
      "metadata": {
        "id": "axCRlFCxQdN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7f15e6-f861-44e3-fba1-9f5efebc9f34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights_input_hidden_prunned shape: (784, 80)\n",
            "bias_input_hidden_prunned shape: (1, 80)\n",
            "weights_hidden_output_prunned shape: (80, 10)\n",
            "bias_hidden_output_prunned shape: (1, 10)\n",
            "(784, 80) (1, 80) (80, 10) (1, 10)\n",
            "Precision del modelo sin cuantificar y prunning 10% 11.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned model with 30%\n",
        "weights_input_hidden_p30, bias_input_hidden_p30, weights_hidden_output_p30, bias_hidden_output_p30 = prune_model(weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, 0.3)\n",
        "\n",
        "#Crear un nuevo objeto\n",
        "nn_mnist_p30 = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "nn_mnist_p30.weights_input_hidden = weights_input_hidden_p30\n",
        "nn_mnist_p30.bias_input_hidden_p30 = bias_input_hidden_p30\n",
        "nn_mnist_p30.weights_hidden_output_p30 = weights_hidden_output_p30\n",
        "nn_mnist_p30.bias_hidden_output_p30 = bias_hidden_output_p30\n",
        "\n",
        "print(weights_input_hidden_p30.shape, bias_input_hidden_p30.shape, weights_hidden_output_p30.shape, bias_hidden_output_p30.shape)\n",
        "#test model\n",
        "y_pred_p30 = nn_mnist_p30.forward(x_test)\n",
        "y_pred_p30 = nn_mnist_p30.active_neuron(y_pred_p30)\n",
        "\n",
        "#Computation error\n",
        "error = y_test - y_pred_p30\n",
        "print(\"Precision del modelo sin cuantificar y prunning 30%\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ],
      "metadata": {
        "id": "CSlaoPnsTMq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df5c0e0-7f13-420e-b816-3e439760c1ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights_input_hidden_prunned shape: (784, 80)\n",
            "bias_input_hidden_prunned shape: (1, 80)\n",
            "weights_hidden_output_prunned shape: (80, 10)\n",
            "bias_hidden_output_prunned shape: (1, 10)\n",
            "(784, 80) (1, 80) (80, 10) (1, 10)\n",
            "Precision del modelo sin cuantificar y prunning 30% 10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruned model with 50%\n",
        "weights_input_hidden_p50, bias_input_hidden_p50, weights_hidden_output_p50, bias_hidden_output_p50= prune_model(weights_input_hidden, bias_input_hidden, weights_hidden_output, bias_hidden_output, 0.5)\n",
        "\n",
        "#Crear un nuevo objeto\n",
        "nn_mnist_p50 = NeuralNetwork(input_size=784, hidden_size=80, output_size=10)\n",
        "nn_mnist_p50.weights_input_hidden = weights_input_hidden_p50\n",
        "nn_mnist_p50.bias_input_hidden_p50 = bias_input_hidden_p50\n",
        "nn_mnist_p50.weights_hidden_output_p50 = weights_hidden_output_p50\n",
        "nn_mnist_p50.bias_hidden_output_p50 = bias_hidden_output_p50\n",
        "\n",
        "#test model\n",
        "y_pred_p50 = nn_mnist_p50.forward(x_test)\n",
        "y_pred_p50 = nn_mnist_p50.active_neuron(y_pred_p50)\n",
        "\n",
        "#Computation error\n",
        "error = y_test - y_pred_p50\n",
        "print(\"Precision del modelo sin cuantificar y prunning 50%\",(np.count_nonzero(error==0)*100)/len(y_test))"
      ],
      "metadata": {
        "id": "oiine_ziTtEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8282667b-4654-46e6-f91a-d5cef4d32458"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights_input_hidden_prunned shape: (784, 80)\n",
            "bias_input_hidden_prunned shape: (1, 80)\n",
            "weights_hidden_output_prunned shape: (80, 10)\n",
            "bias_hidden_output_prunned shape: (1, 10)\n",
            "Precision del modelo sin cuantificar y prunning 50% 10.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuLQlI19ItIZ"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "Discuss the following questions based on the lab experiments and the theory studied:\n",
        "\n",
        "\n",
        "*   How is it important to store the weights in binary format?\n",
        "\n",
        "The main reason for using binary format is independent of the machine or hardware architecture, so it can be read and written in a computer processor as in a microcontroller. Furthermore, using other formats with json, txt or csv consume more memory space and resources since they add metadata to the data that carries extra information about its structure and organization.\n",
        "\n",
        "\n",
        "*   How much reduction in model memory requirements can be achieved by each of the versions obtained?\n",
        "\n",
        "1. int8 model: a memory consumption of 64KB was achieved\n",
        "2. int8 with centering model: a memory consumption of 64KB was achieved\n",
        "3. float16 model: memory consumption of 128KB was achieved\n",
        "4. original unquantized model: memory consumption is 509KB\n",
        "\n",
        "\n",
        "\n",
        "|                     | *size memory* | *mean accuracy* |\n",
        "|:-------------------:|:---------------------:|:----------------------:|\n",
        "|      *Float 64*      |        509.4 kB       |           78%          |\n",
        "|      *Float 16*      |        127.7 kB       |           78%          |\n",
        "|   *Naive Int 8*   |        64.1 kB        |           10%          |\n",
        "| *Centering Int 8* |        64.1 kB        |           11%          |\n",
        "\n",
        "Therefore, the reduction in model memory requirements achieved by each version compared to the original unquantized model is as follows:\n",
        "\n",
        "INT8 Model: 445KB\n",
        "INT8 with Centering Model: 445 KB\n",
        "Float16 model: 381 KB\n",
        "These reductions in memory consumption highlight the effectiveness of quantization techniques, such as using INT8 and float16 data types for weights, resulting in significant memory savings but at the same time reducing model accuracy.\n",
        "\n",
        "\n",
        "*   What are the posible computational advantages of the obtained models and how do they depend on the hardware?\n",
        "\n",
        "The computational advantages of the models obtained, particularly through techniques such as quantization and pruning, are essential to improve performance and efficiency in embedded systems. By employing lower precision data types and eliminating redundant connections, quantized and pruned models significantly reduce computational complexity and memory requirements, enabling faster inference times, lower power consumption, and optimal resource utilization. . These advantages are especially critical in embedded systems, where hardware resources are often limited and real-time processing is essential. Optimizing models for embedded systems involves striking a balance between computational efficiency, memory limitations, and real-time performance requirements to achieve optimal results.\n",
        "\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "\n",
        "* The pruning method may generate inconsistent results, since if random weights are set to zero, i.e., the connection is randomly \"turned off\", there may be weights with a high level of relevance. If these are removed, the performance of the model may decrease significantly. To avoid this, a criterion must be established to remove the weights that contribute the least to the model, thus reducing the computational cost without affecting accuracy.\n",
        "\n",
        "* Compression of AI models and quantization of their weights offer significant computational advantages, such as reduced memory footprint and faster inference times. These advantages depend on the hardware used and are especially important for embedded devices with limited memory resources and computing power. However, it is important to keep in mind that quantization can affect the accuracy of the model, so it is necessary to find a balance between efficiency and performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}